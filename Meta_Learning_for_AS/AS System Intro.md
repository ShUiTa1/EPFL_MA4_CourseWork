# 基于 Meta-RL 与 MPC 的主动悬架（Active Suspension）控制方案

## 1. 核心研究思路
针对1/4主动悬架系统（包含车身、轮胎、路面三个质量块及直流电机执行器）“结构稳定但动力学参数易变”且“实时性要求极高”的特点，采用 **Meta-RL（元强化学习） + MPC（模型预测控制）** 的融合架构：
* **痛点**：RL 训练容易越界损坏硬件；MPC 在线计算过慢无法满足高频控制。
* **解决方案**：在**训练阶段**引入 MPC 保障安全并加速采样；在**测试（实施）阶段**关闭 MPC，仅用 Meta-RL 网络进行极速前向推理控制。

---

## 2. 系统建模与需求转化为数学语言
定义系统核心状态变量：$x_s$ (簧上质量位移)、$x_u$ (簧下质量位移)、$x_r$ (路面位移)、$u$ (电机主动控制力)。

### 2.1 优化目标 (Cost/Reward 函数)
* **乘坐舒适性 (Ride Comfort)**：最小化车身垂直加速度。
    $$J_{comfort} = \sum_{t=0}^{T} \ddot{x}_s(t)^2$$
* **路面附着力 (Road Holding)**：最小化轮胎动态变形（确保抓地力）。
    $$J_{road} = \sum_{t=0}^{T} (x_u(t) - x_r(t))^2$$

### 2.2 系统硬约束 (Constraints)
* **悬架工作空间 (Workspace)**：防打底机械碰撞限制。
    $$|x_s(t) - x_u(t)| \le d_{max}$$
* **执行器限制 (Actuator Limits)**：直流电机推力/电流上限。
    $$|u(t)| \le F_{max}$$

---

## 3. Meta-Learning 的任务 (Task) 场景定义
在 Meta-learning 框架中，将系统面临的“动态变化”定义为不同的任务分布：

* **质量变化（负载/人数）**：
    簧上质量 $m_s$ 作为隐藏变量，遵循均匀分布 $m_s \sim U(m_{min}, m_{max})$。网络需在单次轨迹中通过历史数据快速推断出当前质量。
* **路面输入（扰动/路况）**：
    由底部电机带动的丝杠系统模拟。在数学上通过 ISO 8608 标准的一阶滤波白噪声建模表示不同等级路面：
    $$\dot{x}_r(t) = -2\pi f_0 v x_r(t) + w(t)$$

---

## 4. 两阶段实施架构设计



### 阶段一：离线元训练 (Offline Meta-Training)
* **环境搭建**：基于物理公式构建精确的 Simulink/Python 悬架仿真模型。
* **MPC 事件触发机制**：当 RL 的探索动作导致系统接近物理极限（如悬架行程即将耗尽、存在碰撞风险）时，以概率 $\varepsilon$ 激活带有 CVaR 约束的 MPC 控制器接管系统。
* **经验回放**：将 MPC 算出的安全动作存入 Transition samples，引导 RL 网络安全、高效地学习。

### 阶段二：在线实施与决策 (Online Meta-Testing)
* **关闭 MPC**：部署到物理台架后，彻底关闭耗时的 MPC 模块，实现 Model-free 运行。
* **毫秒级自适应**：利用训练好的 Meta-RL 神经网络，输入极短的历史时间窗数据（如过去 0.5s 的传感器状态），实时更新潜在上下文变量 (Latent context variable)。
* **输出控制**：通过简单的网络前向计算（微秒级）输出最优阻尼力 $u$，完美满足底盘高频实时控制需求。

## 5. 严格的物理驱动马尔可夫决策过程 (MDP) 定义

针对 Quanser 主动悬架台架，我们将系统每个离散控制周期 $\Delta t$（如 1ms）下的物理状态严格映射为 MDP 四要素：

### 5.1 状态空间 $S_t$ (State)
基于台架的传感器配置，网络每个周期读取到的精确物理量：
* $z_{s,t} - z_{u,t}$: **悬架动挠度**（通过高分辨率编码器读取，反映悬架压缩量）。
* $z_{u,t} - z_{r,t}$: **轮胎动载变形量**（通过编码器计算，反映抓地力）。
* $\dot{z}_{s,t} - \dot{z}_{u,t}$: **悬架相对运动速度**（由位移数值差分得出）。
* $\ddot{z}_{s,t}$: **车身垂直加速度**（由顶板 9.81 m/Vs² 灵敏度的加速度计直接读取）。

### 5.2 动作空间 $A_t$ (Action)
* $u_t$: 输出给 226 W 无刷直流电机 (Brushless DC motor) 的控制力（或等效电流）指令。

### 5.3 奖励函数 $R_t$ (Reward)
综合考虑乘坐舒适度、物理安全边界与执行器寿命，构建严谨的加权负奖励函数：
$$R_t = - \Big( w_{acc} \ddot{z}_{s,t}^2 + w_{jerk} J_{s,t}^2 + w_{sus} (z_{s,t} - z_{u,t})^2 + w_{tire} (z_{u,t} - z_{r,t})^2 + w_{u} u_t^2 + w_{\Delta u} (\Delta u_t)^2 \Big)$$
* **核心舒适度 ($w_{acc}$)**：惩罚车身加速度 $\ddot{z}_{s,t}$，减小人体受力压迫感。
* **高频冲击限制 ($w_{jerk}$)**：利用加速度计读数差分计算加加速度（Jerk） $J_{s,t} = \frac{\ddot{z}_{s,t} - \ddot{z}_{s,t-1}}{\Delta t}$。这能强迫电机主动发力去抵消由于弹簧和阻尼引起的突变合力，消除机械冲击感。
* **安全防打底 ($w_{sus}$)**：逼迫悬架在冲击后迅速回归平衡位置，严防动挠度突破台架 $\pm 25.4$ mm 的物理极限。
* **道路附着力 ($w_{tire}$)**：稳定动态轮胎力，防止跳轮导致失控。
* **执行器平滑与寿命 ($w_{\Delta u}$ & $w_u$)**：限制电机指令跳变 $\Delta u_t = u_t - u_{t-1}$ 以及绝对输出。这能保护 226 W 电机免受高频大电流脉冲烧毁，并消除机械传动的异响。

---

## 6. Meta-Learning 任务分布与极速上下文自适应


### 6.1 任务分布定义 (Task Distribution, $\mathcal{T}$)
利用台架的硬件可调特性，在物理层面定义系统动力学的“未知变化”：
* **簧上质量 $m_s$**：通过台架顶板的可拆卸配重 (Removable weight unit) 改变。
* **悬架刚度 $k_s$**：通过可调弹簧机制在 0.4 到 2 N/mm 之间改变。
* **任务 $T_i$**：在每次训练或实车测试时，系统所处的特定物理参数组合 $T_i = (m_s, k_s)$。网络在训练前对此一无所知。

### 6.2 在线上下文推断 (Context Inference)
在实车无模型 (Model-free) 运行阶段，网络通过以下机制实现单次轨迹 (Single trajectory) 内的瞬间自适应：
1. **历史滑窗收集**：连续收集过去 $H$ 个控制周期（如过去 0.2s）的传感器数据序列 $c_t = \{ (s_\tau, a_\tau, r_\tau, s_{\tau+1}) \}_{\tau=t-H}^{t}$。
2. **潜变量生成**：上下文编码器 (Context Encoder) 将这段包含震动反馈的高维时序数据 $c_t$ 压缩为一个低维的**潜在上下文变量 $Z$**。此时，网络在数学上隐式地“感知”到了当前的负载重量和弹簧刚度。
3. **策略执行**：Actor 控制网络 $\pi(S_t, Z)$ 结合当前的瞬时状态 $S_t$ 与推断出的环境特征 $Z$，瞬间前向计算出当前最优的电机控制力 $u_t$。